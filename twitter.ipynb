{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "de3b40d7-3b52-4636-9a1f-39817fffd46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import datetime as dt\n",
    "import time\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "986e2e28-eb1a-4bf8-b4d2-33f692e29f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/yhacoupian/git_repos/project_2\n"
     ]
    }
   ],
   "source": [
    "print(Path.cwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8c4493bd-5b1b-4c4e-81c2-d3c7e60ddc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init():\n",
    "    \"\"\"Sets the initial parameters for the application\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        None: This function does not have any input parameters.\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "        None: This function does not return any value.\n",
    "\n",
    "   \"\"\"\n",
    "    # Set the path for the data collection directory\n",
    "    global data_dir\n",
    "    data_dir = Path.cwd() / 'Data'\n",
    "    \n",
    "    # Create the directory if it does not exist\n",
    "    try:\n",
    "        os.makedirs(data_dir)    \n",
    "        print(\"Directory \" , data_dir ,  \" Created \")\n",
    "    except FileExistsError:\n",
    "        print(\"Directory \" , data_dir ,  \" already exists\")  \n",
    "    \n",
    "    # Max number of tweets to be scrapped per day\n",
    "    global maxTweets\n",
    "    maxTweets = 500\n",
    "    \n",
    "    # Setup the sentiment dataframe \n",
    "    column_names = [\"date\", \"neg\", \"neu\", \"pos\", \"compound\"]\n",
    "    df = pd.DataFrame(columns = column_names)\n",
    "    \n",
    "    # Set sentiment file path\n",
    "    global sentiment_path\n",
    "    sentiment_path = Path.joinpath(data_dir,  'sentiment.csv') \n",
    "    \n",
    "    # Create the CSV file with proper columns\n",
    "    df.to_csv(sentiment_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5ef459f1-769d-4e15-bb0e-67ca3a24e459",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_sentiment(date, neg, neu, pos, compound):\n",
    "    \"\"\"Writes the sentiment analysis results into the sentiment.csv file\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        date: This is the date corresponding to the day the sentiments belong to.\n",
    "        neg: This is the value for the negative sentiment. \n",
    "        neu: This is the value for the neutral sentiment.\n",
    "        pos: This is the value for the positive sentiment.\n",
    "        compound: This is the compounded value.\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "        None: This function does not return any value.\n",
    "\n",
    "   \"\"\"\n",
    "    try:\n",
    "        sentiment_df = pd.read_csv(sentiment_path)\n",
    "        # Create a new row \n",
    "        new_row = {'date':date, 'neg':neg, 'neu':neu, 'pos':pos, 'compound':compound}\n",
    "        \n",
    "        # Add the new row to the dataframe \n",
    "        sentiment_df.loc[len(sentiment_df)]= new_row\n",
    "    \n",
    "        # Update the csv file\n",
    "        sentiment_df.to_csv(sentiment_path, index= False)\n",
    "    except Exception as err:\n",
    "        print(f\"Error: '{err}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f64e1b40-b9e5-43ab-a706-54d80355c4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_per_day(keyword, start_date, end_date):\n",
    "    \"\"\"Uses the snscrape to go through tweets from the start_date to the end_date looking for the keyword. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        keyword: This is the phrase to look for in the tweets.\n",
    "        start_date: The start date. \n",
    "        end_date: The end date.\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "        None: This function does not return any value.\n",
    "\n",
    "   \"\"\"\n",
    "    \n",
    "    # Set the path\n",
    "    unique_file_name = keyword +'-sentiment-' + end_date + '.csv'\n",
    "    daily_result_path = Path.joinpath(data_dir, unique_file_name)\n",
    "    \n",
    "    #Open/create a file to append data to\n",
    "    csvFile = open(daily_result_path, 'a', newline='', encoding='utf8')\n",
    "    \n",
    "    #Use csv writer\n",
    "    csvWriter = csv.writer(csvFile)\n",
    "    csvWriter.writerow(['id','date','tweet',])\n",
    "    \n",
    "    # set the batch number\n",
    "    batch = 1000\n",
    "\n",
    "    print(f'Each batch is equal to {batch} tweet(s).')\n",
    "    print(\"Scrapping...\")\n",
    "    try:\n",
    "        count = 1\n",
    "        for i,tweet in enumerate(sntwitter.TwitterSearchScraper(keyword + ' lang:en since:' +  start_date + ' until:' + end_date + ' -filter:links -filter:replies').get_items()):\n",
    "            if i > maxTweets :\n",
    "                break\n",
    "            csvWriter.writerow([tweet.id, tweet.date, tweet.content])\n",
    "        \n",
    "            # Print a message for every 1000 tweets scrapped. \n",
    "            if i % batch == 0:\n",
    "                print('batch =', end=f' {count} | ')\n",
    "                count+=1\n",
    "            \n",
    "        csvFile.close()\n",
    "    except Exception as err:\n",
    "        print(f\"Error: '{err}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ed69c6d9-87e5-4261-b4bc-3e3a0c8ef8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def anlayze_vader(keyword, end_date):\n",
    "    \"\"\"Uses the VADERsentiment to analyze the collected data based on a given keyword. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        keyword: This is the phrase to analyze its sentiment.. \n",
    "        end_date: used to find the file name to analyze.\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "        None: This function does not return any value.\n",
    "\n",
    "   \"\"\"\n",
    "    print(\"Analyzing...\")\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    # Construct the path \n",
    "    unique_file_name = keyword +'-sentiment-' + end_date + '.csv'\n",
    "    daily_result_path = Path.joinpath(data_dir , unique_file_name)\n",
    "    \n",
    "    # Reading the CSV file \n",
    "    df = pd.read_csv(daily_result_path, parse_dates=True, index_col=0)\n",
    "\n",
    "    # Creating sentiment scores columns\n",
    "    df['compound'] = [analyzer.polarity_scores(x)['compound'] for x in df['tweet']]\n",
    "    df['neg'] = [analyzer.polarity_scores(x)['neg'] for x in df['tweet']]\n",
    "    df['neu'] = [analyzer.polarity_scores(x)['neu'] for x in df['tweet']]\n",
    "    df['pos'] = [analyzer.polarity_scores(x)['pos'] for x in df['tweet']]\n",
    "\n",
    "    # Taking averages of sentiment score columns\n",
    "    avg_compound = np.average(df['compound'])\n",
    "    avg_neg = np.average(df['neg']) * -1  # Change neg value to negative number for clarity\n",
    "    avg_neu = np.average(df['neu'])\n",
    "    avg_pos = np.average(df['pos'])\n",
    "\n",
    "    # Counting number of tweets\n",
    "    count = len(df.index)\n",
    "\n",
    "    # Write to the sentiment csv file\n",
    "    write_sentiment(end_date, avg_neg, avg_neu, avg_pos, avg_compound)\n",
    "    \n",
    "    print(\"Done!\")\n",
    "    print('-'*20)\n",
    "    # Print Statements\n",
    "    print(\"For the given period there has been\", count ,  \"tweets on \" + keyword, end='\\n*')\n",
    "    print(\"Positive Sentiment:\", '%.2f' % avg_pos, end='\\n*')\n",
    "    print(\"Neutral Sentiment:\", '%.2f' % avg_neu, end='\\n*')\n",
    "    print(\"Negative Sentiment:\", '%.2f' % avg_neg, end='\\n*')\n",
    "    print(\"Compound Sentiment:\", '%.2f' % avg_compound, end='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9baaf7-5d66-4414-8b2d-83d881dd50f2",
   "metadata": {},
   "source": [
    "### Scrap and Analyze using VADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e03794f0-c381-4cf8-95ea-857263e21229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory  /Users/yhacoupian/git_repos/project_2/Data  already exists\n"
     ]
    }
   ],
   "source": [
    "init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c914bf11-5741-4b0d-9686-41babade84ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the start date\n",
    "start_date = dt.date(2021, 3, 1)\n",
    "\n",
    "# set the end date\n",
    "end_date = dt.date(2021, 5, 1)\n",
    "\n",
    "# set the delta - currently set to one day\n",
    "delta = dt.timedelta(days=1)\n",
    "# specify the keword\n",
    "word = 'wuhan'\n",
    "\n",
    "# go through the days to scrap and analyze each day\n",
    "while start_date < end_date:\n",
    "    print(start_date, end=' : ')\n",
    "    scrap_per_day(word, start_date.strftime('%Y-%m-%d'), (start_date + delta).strftime('%Y-%m-%d'))\n",
    "    anlayze_vader(word, (start_date + delta).strftime('%Y-%m-%d'))\n",
    "    start_date += delta \n",
    "    print('-'*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "85f34ce3-1440-4a6d-952b-3e165e7a766d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          date       neg       neu       pos  compound Sentiment\n",
      "0   2021-03-02 -0.099973  0.829761  0.070257 -0.094956       neg\n",
      "1   2021-03-03 -0.075642  0.856706  0.067652 -0.004002       neg\n",
      "2   2021-03-04 -0.100769  0.824382  0.074869 -0.098311       neg\n",
      "3   2021-03-05 -0.097500  0.829856  0.072609 -0.070699       neg\n",
      "4   2021-03-06 -0.093108  0.843884  0.063017 -0.131457       neg\n",
      "..         ...       ...       ...       ...       ...       ...\n",
      "56  2021-04-27 -0.094392  0.823961  0.081621 -0.060069       neg\n",
      "57  2021-04-28 -0.104866  0.830457  0.064683 -0.151950       neg\n",
      "58  2021-04-29 -0.105913  0.820312  0.073780 -0.103406       neg\n",
      "59  2021-04-30 -0.085856  0.840331  0.073806 -0.061213       neg\n",
      "60  2021-05-01 -0.085559  0.838571  0.075853 -0.058898       neg\n",
      "\n",
      "[61 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# write the final sentiment column to the sentiment.csv\n",
    "sentiment_df = pd.read_csv(sentiment_path)\n",
    "sentiment_df['Sentiment'] = sentiment_df['compound'].apply(lambda c: 'neu' if c==0 else 'pos' if c>0 else 'neg')\n",
    "print(sentiment_df)\n",
    "# Update the csv file\n",
    "sentiment_df.to_csv(sentiment_path, index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69811fd-207a-42b3-96c7-3dc68f9db40b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
